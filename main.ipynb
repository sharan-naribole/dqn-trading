{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Trading System - Main Orchestration Notebook\n",
    "\n",
    "This notebook orchestrates the complete DQN trading workflow:\n",
    "1. Data collection and preprocessing (one-time per project)\n",
    "2. Training for each strategy config\n",
    "3. Out-of-sample validation comparison across all strategies\n",
    "4. Final test comparison across all strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import modules\n",
    "from src.utils.config_loader import ConfigLoader\n",
    "from src.utils.model_manager import ModelManager\n",
    "from src.utils.progress_logger import ProgressLogger\n",
    "from src.data.collector import DataCollector\n",
    "from src.data.splitter import DataSplitter\n",
    "from src.features.engineer import FeatureEngineer\n",
    "from src.features.normalizer import RollingNormalizer\n",
    "from src.trading.environment import TradingEnvironment\n",
    "from src.training.trainer import DQNTrainer\n",
    "from src.evaluation.validator import OutOfSampleValidator\n",
    "from src.evaluation.metrics import PerformanceMetrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "### Project Folder Structure:\n",
    "- Each project folder contains:\n",
    "  - `data_config.json` - Data settings, ticker, dates, validation/test splits\n",
    "  - `trading_*.json` - One or more trading strategy configs\n",
    "\n",
    "### Available Projects:\n",
    "- `dry_run` - Quick test (1 episode, 2 years data)\n",
    "- `default_run` - Full training (100 episodes, 10 years data)\n",
    "\n",
    "### Test Mode:\n",
    "- `test_mode=True` - Skip training if models exist, otherwise assert\n",
    "- `test_mode=False` - Always perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "PROJECT_FOLDER = 'my_project'  # Options: 'dry_run', 'default_run', or custom folder name\n",
    "TEST_MODE = False  # If True, skip training if models exist (for testing only)\n",
    "# ===================================\n",
    "\n",
    "# Build paths\n",
    "project_path = f'config/{PROJECT_FOLDER}'\n",
    "data_config_path = f'{project_path}/data_config.json'\n",
    "\n",
    "# Verify project folder exists\n",
    "if not os.path.exists(project_path):\n",
    "    raise ValueError(f\"Project folder not found: {project_path}\")\n",
    "\n",
    "if not os.path.exists(data_config_path):\n",
    "    raise ValueError(f\"Data config not found: {data_config_path}\")\n",
    "\n",
    "# Load data config\n",
    "data_config = ConfigLoader(data_config_path)\n",
    "\n",
    "# Find all trading config files\n",
    "trading_configs_paths = glob.glob(f'{project_path}/trading_*.json')\n",
    "if not trading_configs_paths:\n",
    "    raise ValueError(f\"No trading configs found in {project_path}/\")\n",
    "\n",
    "# Sort for consistent ordering\n",
    "trading_configs_paths.sort()\n",
    "\n",
    "# Load all trading configs\n",
    "trading_configs = []\n",
    "for path in trading_configs_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        tc = json.load(f)\n",
    "        trading_configs.append({\n",
    "            'path': path,\n",
    "            'config': tc,\n",
    "            'strategy_name': tc.get('strategy_name', tc.get('experiment_name', 'Unnamed')),\n",
    "            'experiment_name': tc.get('experiment_name', os.path.basename(path).replace('.json', ''))\n",
    "        })\n",
    "\n",
    "# Initialize run\n",
    "ticker = data_config.config['ticker']\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f\"{PROJECT_FOLDER}_{ticker}_{timestamp}\"\n",
    "logger = ProgressLogger(run_name=run_name, log_dir=f\"logs/{PROJECT_FOLDER}\")\n",
    "\n",
    "# Create results directory early for saving plots\n",
    "results_dir = f\"results/{PROJECT_FOLDER}/run_{timestamp}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üöÄ DQN Trading System\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üìÅ Project: {PROJECT_FOLDER}\")\n",
    "print(f\"üìä Ticker: {ticker}\")\n",
    "print(f\"üìÖ Date range: {data_config.config['start_date']} to {data_config.config['end_date']}\")\n",
    "print(f\"üéØ Strategies: {len(trading_configs)}\")\n",
    "for i, tc in enumerate(trading_configs, 1):\n",
    "    print(f\"   {i}. {tc['strategy_name']} ({tc['experiment_name']})\")\n",
    "print(f\"üß™ Test mode: {'ON' if TEST_MODE else 'OFF'}\")\n",
    "print(f\"üìù Run name: {run_name}\")\n",
    "print(f\"üìÇ Results will be saved to: {results_dir}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nMonitor this run: python monitor_training.py --run {run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection (One-Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data once for all strategies\n",
    "logger.start_stage(\"Data Collection\", f\"Collecting {ticker} and VIX data\")\n",
    "\n",
    "collector = DataCollector(data_config.config)\n",
    "spy_data, vix_data = collector.collect_data(force_download=False)\n",
    "\n",
    "info = collector.get_data_info(spy_data, vix_data)\n",
    "logger.complete_stage(\"Data Collection\", info)\n",
    "\n",
    "print(\"‚úÖ Data Collection Summary:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data and create features\n",
    "logger.start_stage(\"Feature Engineering\", f\"Creating technical indicators and features\")\n",
    "\n",
    "combined_data = collector.combine_data(spy_data, vix_data)\n",
    "print(f\"Combined data shape: {combined_data.shape}\")\n",
    "\n",
    "engineer = FeatureEngineer(data_config.config)\n",
    "featured_data = engineer.create_features(combined_data)\n",
    "\n",
    "feature_columns = engineer.get_feature_names(featured_data)\n",
    "feature_info = engineer.get_feature_info(featured_data)\n",
    "\n",
    "metrics = {\n",
    "    'total_features': len(feature_columns),\n",
    "    'price_features': len(feature_info['price_features']),\n",
    "    'technical_indicators': len(feature_info['technical_indicators']),\n",
    "    'volume_features': len(feature_info['volume_features']),\n",
    "    'vix_features': len(feature_info['vix_features'])\n",
    "}\n",
    "\n",
    "logger.complete_stage(\"Feature Engineering\", metrics)\n",
    "\n",
    "print(f\"\\n‚úÖ Total features: {len(feature_columns)}\")\n",
    "print(f\"Feature categories:\")\n",
    "for category in ['price_features', 'technical_indicators', 'volume_features', 'vix_features']:\n",
    "    print(f\"  ‚Ä¢ {category}: {len(feature_info[category])} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data FIRST (before normalization to avoid lookahead bias)\n",
    "logger.start_stage(\"Data Splitting\", \"Creating train/validation/test splits\")\n",
    "\n",
    "splitter = DataSplitter(data_config.config)\n",
    "splits = splitter.split_data(featured_data, verbose=True)\n",
    "\n",
    "train_data_raw = splits['train']\n",
    "validation_periods_raw = splits['validation']\n",
    "test_data_raw = splits['test']\n",
    "\n",
    "print(f\"\\n‚úÖ Data split complete (unnormalized):\")\n",
    "print(f\"  ‚Ä¢ Training samples: {len(train_data_raw)}\")\n",
    "print(f\"  ‚Ä¢ Validation periods: {len(validation_periods_raw)}\")\n",
    "print(f\"  ‚Ä¢ Test samples: {len(test_data_raw)}\")\n",
    "\n",
    "# Continuous timeline normalization\n",
    "print(f\"\\nüìä Normalizing data (continuous timeline, stateful rolling)...\")\n",
    "logger.start_stage(\"Data Normalization\", \"Applying continuous rolling Z-score normalization\")\n",
    "\n",
    "all_data_chronological = pd.concat([\n",
    "    train_data_raw,\n",
    "    *validation_periods_raw,\n",
    "    test_data_raw\n",
    "]).sort_index()\n",
    "\n",
    "print(f\"  ‚Ä¢ Chronological timeline: {len(all_data_chronological)} total samples\")\n",
    "print(f\"  ‚Ä¢ Date range: {all_data_chronological.index.min().date()} to {all_data_chronological.index.max().date()}\")\n",
    "\n",
    "normalizer = RollingNormalizer(data_config.config)\n",
    "all_data_normalized = normalizer.fit_transform(\n",
    "    all_data_chronological,\n",
    "    feature_columns,\n",
    "    preserve_original=True\n",
    ")\n",
    "\n",
    "print(f\"  ‚úì Applied rolling normalization (window={data_config.config['data']['normalization_window']} days)\")\n",
    "\n",
    "# Extract normalized splits\n",
    "train_data = all_data_normalized.loc[train_data_raw.index]\n",
    "validation_periods = [\n",
    "    all_data_normalized.loc[val_period.index]\n",
    "    for val_period in validation_periods_raw\n",
    "]\n",
    "test_data = all_data_normalized.loc[test_data_raw.index]\n",
    "\n",
    "logger.complete_stage(\"Data Normalization\", {\n",
    "    'train_shape': train_data.shape,\n",
    "    'validation_periods': len(validation_periods),\n",
    "    'test_shape': test_data.shape\n",
    "})\n",
    "\n",
    "logger.complete_stage(\"Data Splitting\", {\n",
    "    'training_samples': len(train_data),\n",
    "    'validation_periods': len(validation_periods),\n",
    "    'test_samples': len(test_data)\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Normalized data shapes:\")\n",
    "print(f\"  ‚Ä¢ Train: {train_data.shape}\")\n",
    "print(f\"  ‚Ä¢ Validation: {len(validation_periods)} periods\")\n",
    "print(f\"  ‚Ä¢ Test: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop - All Strategies\n",
    "\n",
    "Train a model for each trading config in the project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_results = {}\n",
    "model_manager = ModelManager(base_dir=\"models\")\n",
    "\n",
    "for idx, tc_info in enumerate(trading_configs, 1):\n",
    "    strategy_name = tc_info['strategy_name']\n",
    "    experiment_name = tc_info['experiment_name']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä Strategy {idx}/{len(trading_configs)}: {strategy_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Merge data config with trading config\n",
    "    full_config = {**data_config.config, **tc_info['config']}\n",
    "\n",
    "    # Create training environment\n",
    "    train_env = TradingEnvironment(\n",
    "        train_data,\n",
    "        feature_columns,\n",
    "        full_config,\n",
    "        mode='train'\n",
    "    )\n",
    "\n",
    "    # Build model identifier (mirroring config structure)\n",
    "    model_identifier = f\"{PROJECT_FOLDER}/{experiment_name}\"\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = DQNTrainer(full_config, model_manager, progress_logger=logger)\n",
    "\n",
    "    # Check for existing model (for TEST_MODE)\n",
    "    model_dir = f\"models/{model_identifier}\"\n",
    "    existing_models = glob.glob(f\"{model_dir}/*.h5\") if os.path.exists(model_dir) else []\n",
    "\n",
    "    # Training logic\n",
    "    if TEST_MODE:\n",
    "        if existing_models:\n",
    "            # Find the latest model\n",
    "            latest_model = sorted(existing_models)[-1]\n",
    "            print(f\"‚úÖ Test mode: Loading existing model from {latest_model}\")\n",
    "            trainer.agent.model.load_weights(latest_model)\n",
    "            training_history = {'training_history': []}\n",
    "            print(f\"   Model loaded successfully\")\n",
    "        else:\n",
    "            raise AssertionError(f\"Test mode enabled but no models found in {model_dir}\")\n",
    "    else:\n",
    "        # Train model\n",
    "        logger.start_stage(f\"Training: {strategy_name}\",\n",
    "                          f\"Training for {full_config['training']['episodes']} episodes\")\n",
    "\n",
    "        print(f\"Starting training...\")\n",
    "        print(f\"üìÅ Models will be saved to: {model_dir}/\")\n",
    "        print(f\"üíæ Save frequency: Every {full_config['training']['save_frequency']} episodes\")\n",
    "        print(f\"Monitor progress: python monitor_training.py --run {run_name}\\n\")\n",
    "\n",
    "        # The trainer will automatically save models based on save_frequency\n",
    "        training_history = trainer.train(train_env, validation_env=None, verbose=True)\n",
    "\n",
    "        logger.complete_stage(f\"Training: {strategy_name}\", {\n",
    "            'episodes_completed': len(training_history['training_history']),\n",
    "            'final_return': training_history['training_history'][-1]['return'] if training_history['training_history'] else 0\n",
    "        })\n",
    "\n",
    "        print(f\"\\n‚úÖ Training complete for {strategy_name}\")\n",
    "\n",
    "    # Store results\n",
    "    all_results[strategy_name] = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'trainer': trainer,\n",
    "        'training_history': training_history,\n",
    "        'config': full_config,\n",
    "        'model_identifier': model_identifier\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ ALL TRAINING COMPLETE - {len(trading_configs)} strategies\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress for all strategies\n",
    "if not TEST_MODE:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "    for strategy_name, results in all_results.items():\n",
    "        history = results['training_history']['training_history']\n",
    "        if history:\n",
    "            history_df = pd.DataFrame(history)\n",
    "\n",
    "            # Episode rewards\n",
    "            axes[0, 0].plot(history_df['episode'], history_df['reward'],\n",
    "                          label=strategy_name, linewidth=2)\n",
    "            axes[0, 0].set_title('Episode Rewards')\n",
    "            axes[0, 0].set_xlabel('Episode')\n",
    "            axes[0, 0].set_ylabel('Total Reward')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].legend()\n",
    "\n",
    "            # Total returns\n",
    "            axes[0, 1].plot(history_df['episode'], history_df['return'] * 100,\n",
    "                          label=strategy_name, linewidth=2)\n",
    "            axes[0, 1].set_title('Total Returns')\n",
    "            axes[0, 1].set_xlabel('Episode')\n",
    "            axes[0, 1].set_ylabel('Return (%)')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            axes[0, 1].legend()\n",
    "\n",
    "            # Training Loss\n",
    "            axes[0, 2].plot(history_df['episode'], history_df.get('avg_loss', 0),\n",
    "                          label=strategy_name, linewidth=2)\n",
    "            axes[0, 2].set_title('Training Loss')\n",
    "            axes[0, 2].set_xlabel('Episode')\n",
    "            axes[0, 2].set_ylabel('Average Loss')\n",
    "            axes[0, 2].grid(True, alpha=0.3)\n",
    "            axes[0, 2].legend()\n",
    "\n",
    "            # Win rate\n",
    "            axes[1, 0].plot(history_df['episode'], history_df['win_rate'] * 100,\n",
    "                          label=strategy_name, linewidth=2)\n",
    "            axes[1, 0].set_title('Win Rate')\n",
    "            axes[1, 0].set_xlabel('Episode')\n",
    "            axes[1, 0].set_ylabel('Win Rate (%)')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].legend()\n",
    "\n",
    "            # Number of trades\n",
    "            axes[1, 1].plot(history_df['episode'], history_df['trades'],\n",
    "                          label=strategy_name, linewidth=2)\n",
    "            axes[1, 1].set_title('Number of Trades')\n",
    "            axes[1, 1].set_xlabel('Episode')\n",
    "            axes[1, 1].set_ylabel('Trades')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            axes[1, 1].legend()\n",
    "\n",
    "            # Sharpe Ratio\n",
    "            axes[1, 2].plot(history_df['episode'], history_df['sharpe'],\n",
    "                          label=strategy_name, linewidth=2)\n",
    "            axes[1, 2].set_title('Sharpe Ratio')\n",
    "            axes[1, 2].set_xlabel('Episode')\n",
    "            axes[1, 2].set_ylabel('Sharpe')\n",
    "            axes[1, 2].grid(True, alpha=0.3)\n",
    "            axes[1, 2].legend()\n",
    "\n",
    "    plt.suptitle(f'Training Progress - {PROJECT_FOLDER}', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{results_dir}/training_progress.png\"\n",
    "    plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Training progress plot saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"üìä Skipping training plots (TEST_MODE enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Out-of-Sample Validation Comparison\n",
    "\n",
    "Compare all strategies across all validation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all strategies on all periods\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìä OUT-OF-SAMPLE VALIDATION COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Validation periods: {len(validation_periods)}\\n\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for strategy_name, results in all_results.items():\n",
    "    print(f\"\\nValidating: {strategy_name}\")\n",
    "    trainer = results['trainer']\n",
    "\n",
    "    period_metrics = []\n",
    "\n",
    "    for i, val_period in enumerate(validation_periods, 1):\n",
    "        print(f\"  Period {i}: {val_period.index.min().date()} to {val_period.index.max().date()}\", end=\" \")\n",
    "\n",
    "        val_env = TradingEnvironment(\n",
    "            val_period,\n",
    "            feature_columns,\n",
    "            results['config'],\n",
    "            mode='test'\n",
    "        )\n",
    "\n",
    "        val_metrics = trainer.evaluate(val_env, verbose=False)\n",
    "        period_metrics.append({\n",
    "            'period': i,\n",
    "            'return': val_metrics.get('total_return', 0),\n",
    "            'sharpe': val_metrics.get('sharpe_ratio', 0),\n",
    "            'max_drawdown': val_metrics.get('max_drawdown', 0),\n",
    "            'win_rate': val_metrics.get('win_rate', 0),\n",
    "            'trades': val_metrics.get('num_trades', 0)\n",
    "        })\n",
    "\n",
    "        print(f\"‚Üí Return: {val_metrics.get('total_return', 0):.2%}\")\n",
    "\n",
    "    validation_results[strategy_name] = period_metrics\n",
    "\n",
    "print(f\"\\n‚úÖ Validation complete for all strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation comparison\n",
    "if len(validation_periods) > 0 and len(all_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    metrics_to_plot = [\n",
    "        ('return', 'Returns by Period (%)', 0, 0, 100),\n",
    "        ('sharpe', 'Sharpe Ratio by Period', 0, 1, 1),\n",
    "        ('win_rate', 'Win Rate by Period (%)', 1, 0, 100),\n",
    "        ('trades', 'Trades by Period', 1, 1, 1)\n",
    "    ]\n",
    "\n",
    "    for metric, title, row, col, multiplier in metrics_to_plot:\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        x = np.arange(1, len(validation_periods) + 1)\n",
    "        width = 0.8 / max(len(all_results), 1)\n",
    "\n",
    "        for i, (strategy_name, period_metrics) in enumerate(validation_results.items()):\n",
    "            values = [pm[metric] * multiplier for pm in period_metrics]\n",
    "            offset = (i - len(all_results)/2 + 0.5) * width\n",
    "            ax.bar(x + offset, values, width, label=strategy_name)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Validation Period')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_xticks(x)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Out-of-Sample Validation Comparison - {PROJECT_FOLDER}', fontsize=14, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = f\"{results_dir}/validation_comparison.png\"\n",
    "    plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n",
    "    print(f\"‚úÖ Validation comparison plot saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"üìä No validation periods to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set Comparison\n",
    "\n",
    "Compare all strategies on the held-out test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all strategies\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üß™ FINAL TEST COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test period: {test_data.index.min().date()} to {test_data.index.max().date()}\")\n",
    "print(f\"Test samples: {len(test_data)}\\n\")\n",
    "\n",
    "test_results = {}\n",
    "metrics_calc = PerformanceMetrics()\n",
    "\n",
    "for strategy_name, results in all_results.items():\n",
    "    print(f\"Testing: {strategy_name}\")\n",
    "    trainer = results['trainer']\n",
    "\n",
    "    test_env = TradingEnvironment(\n",
    "        test_data,\n",
    "        feature_columns,\n",
    "        results['config'],\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "    test_metrics = trainer.evaluate(test_env, verbose=False)\n",
    "    comprehensive_metrics = metrics_calc.calculate_metrics(\n",
    "        test_env.portfolio_values,\n",
    "        test_env.trades,\n",
    "        test_env.starting_balance\n",
    "    )\n",
    "\n",
    "    test_results[strategy_name] = {\n",
    "        'metrics': comprehensive_metrics,\n",
    "        'portfolio_values': test_env.portfolio_values.copy(),\n",
    "        'trades': test_env.trades.copy()\n",
    "    }\n",
    "\n",
    "    print(f\"  Return: {comprehensive_metrics['total_return']:.2%}\")\n",
    "    print(f\"  Sharpe: {comprehensive_metrics['sharpe_ratio']:.2f}\")\n",
    "    print(f\"  Win Rate: {comprehensive_metrics['win_rate']:.2%}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Test evaluation complete for all strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test comparison table\n",
    "comparison_data = []\n",
    "for strategy_name, result in test_results.items():\n",
    "    m = result['metrics']\n",
    "    comparison_data.append({\n",
    "        'Strategy': strategy_name,\n",
    "        'Return (%)': f\"{m['total_return']*100:.2f}\",\n",
    "        'Sharpe': f\"{m['sharpe_ratio']:.2f}\",\n",
    "        'Max DD (%)': f\"{m['max_drawdown']*100:.2f}\",\n",
    "        'Win Rate (%)': f\"{m['win_rate']*100:.2f}\",\n",
    "        'Trades': m['num_trades']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test comparison - Portfolio values over time\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for strategy_name, result in test_results.items():\n",
    "    ax.plot(result['portfolio_values'], label=strategy_name, linewidth=2)\n",
    "\n",
    "# Add buy & hold\n",
    "price_col = f\"{ticker}_Close_orig\"\n",
    "window_size = data_config.config['data']['window_size']\n",
    "initial_price = test_data.iloc[window_size][price_col]\n",
    "final_price = test_data.iloc[-1][price_col]\n",
    "buy_hold_return = (final_price - initial_price) / initial_price\n",
    "starting_balance = list(all_results.values())[0]['config']['trading']['starting_balance']\n",
    "\n",
    "buy_hold_values = [starting_balance * (1 + buy_hold_return * i / len(test_data))\n",
    "                   for i in range(len(test_results[list(test_results.keys())[0]]['portfolio_values']))]\n",
    "ax.plot(buy_hold_values, label='Buy & Hold', linewidth=2, linestyle='--', color='black')\n",
    "\n",
    "ax.set_title(f'Test Period Performance Comparison - {PROJECT_FOLDER}', fontsize=14)\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Portfolio Value ($)')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plot_path = f\"{results_dir}/test_portfolio_values.png\"\n",
    "plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n",
    "print(f\"‚úÖ Test portfolio comparison saved to: {plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison bar chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "strategies = list(test_results.keys())\n",
    "returns = [test_results[s]['metrics']['total_return'] * 100 for s in strategies]\n",
    "sharpes = [test_results[s]['metrics']['sharpe_ratio'] for s in strategies]\n",
    "win_rates = [test_results[s]['metrics']['win_rate'] * 100 for s in strategies]\n",
    "\n",
    "# Add buy & hold to comparison\n",
    "strategies_with_bh = strategies + ['Buy & Hold']\n",
    "returns_with_bh = returns + [buy_hold_return * 100]\n",
    "\n",
    "x = np.arange(len(strategies))\n",
    "x_with_bh = np.arange(len(strategies_with_bh))\n",
    "\n",
    "axes[0].bar(x_with_bh, returns_with_bh, color=['skyblue']*len(strategies) + ['gray'])\n",
    "axes[0].set_title('Total Return (%)')\n",
    "axes[0].set_xticks(x_with_bh)\n",
    "axes[0].set_xticklabels(strategies_with_bh, rotation=45, ha='right')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(x, sharpes, color='orange')\n",
    "axes[1].set_title('Sharpe Ratio')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[2].bar(x, win_rates, color='green')\n",
    "axes[2].set_title('Win Rate (%)')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle(f'Final Test Metrics Comparison - {PROJECT_FOLDER}', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plot_path = f\"{results_dir}/test_metrics_comparison.png\"\n",
    "plt.savefig(plot_path, dpi=100, bbox_inches='tight')\n",
    "print(f\"‚úÖ Test metrics comparison saved to: {plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results data (plots already saved above)\n",
    "# Note: results_dir was created in cell 3\n",
    "\n",
    "# Save CSV comparison\n",
    "comparison_df.to_csv(f\"{results_dir}/test_comparison.csv\", index=False)\n",
    "\n",
    "# Save comprehensive results\n",
    "final_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'project': PROJECT_FOLDER,\n",
    "    'test_mode': TEST_MODE,\n",
    "    'run_name': run_name,\n",
    "    'data_config': data_config.config,\n",
    "    'strategies': {},\n",
    "    'validation_results': validation_results,\n",
    "    'test_results': {k: v['metrics'] for k, v in test_results.items()},\n",
    "    'buy_hold_return': float(buy_hold_return)\n",
    "}\n",
    "\n",
    "for strategy_name, results in all_results.items():\n",
    "    final_results['strategies'][strategy_name] = {\n",
    "        'experiment_name': results['experiment_name'],\n",
    "        'config': results['config'],\n",
    "        'training_history': results['training_history']['training_history']\n",
    "    }\n",
    "\n",
    "# Save to JSON\n",
    "results_path = f\"{results_dir}/final_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ COMPLETE PIPELINE FINISHED!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"üìÅ Project: {PROJECT_FOLDER}\")\n",
    "print(f\"üìù Run name: {run_name}\")\n",
    "print(f\"üìä Strategies trained: {len(trading_configs)}\")\n",
    "print(f\"üìà Results saved to: {results_dir}/\")\n",
    "print(f\"üìÑ Final results: {results_path}\")\n",
    "\n",
    "# List saved files\n",
    "saved_files = os.listdir(results_dir)\n",
    "saved_files.sort()\n",
    "print(f\"\\nüìÅ Saved files:\")\n",
    "for file in saved_files:\n",
    "    size_kb = os.path.getsize(f\"{results_dir}/{file}\") / 1024\n",
    "    file_type = \"üìä Plot\" if file.endswith('.png') else \"üìÑ Data\" if file.endswith('.csv') else \"üìã Results\"\n",
    "    print(f\"   {file_type}: {file} ({size_kb:.1f} KB)\")\n",
    "\n",
    "# Count plots saved\n",
    "plot_files = [f for f in saved_files if f.endswith('.png')]\n",
    "print(f\"\\nüìä Plots saved: {len(plot_files)}\")\n",
    "\n",
    "# Show best strategy\n",
    "if test_results:\n",
    "    print(f\"\\nüèÜ Best performing strategy:\")\n",
    "    best_strategy = max(test_results.items(), key=lambda x: x[1]['metrics']['total_return'])\n",
    "    print(f\"   {best_strategy[0]}\")\n",
    "    print(f\"   ‚Ä¢ Return: {best_strategy[1]['metrics']['total_return']:.2%}\")\n",
    "    print(f\"   ‚Ä¢ Sharpe: {best_strategy[1]['metrics']['sharpe_ratio']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ vs Buy & Hold: {(best_strategy[1]['metrics']['total_return'] - buy_hold_return):.2%}\")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Print final summary\n",
    "logger.print_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
