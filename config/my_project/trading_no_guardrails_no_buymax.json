{
  "_comment": "No guardrails, no BUY_MAX - most constrained action space",
  "experiment_name": "no_guardrails_no_buymax",
  "strategy_name": "No Guardrails (No BUY_MAX)",
  "description": "Pure DQN decision-making without stop-loss, take-profit, or BUY_MAX action",

  "trading": {
    "share_increments": [10, 50, 100, 200],
    "enable_buy_max": false,
    "starting_balance": 100000,
    "idle_reward": -0.001,
    "buy_reward_per_share": 0.0,
    "buy_transaction_cost_per_share": 0.01,
    "sell_transaction_cost_per_share": 0.01,
    "stop_loss_pct": 100,
    "take_profit_pct": 10000
  },

  "network": {
    "architecture": "dueling",
    "shared_layers": [256, 128],
    "value_layers": [128],
    "advantage_layers": [128],
    "activation": "relu",
    "dropout_rate": 0.0,
    "batch_norm": false
  },

  "training": {
    "episodes": 500,
    "batch_size": 64,
    "replay_buffer_size": 10000,
    "target_update_freq": 5,
    "epsilon_start": 1.0,
    "epsilon_end": 0.01,
    "epsilon_decay": 0.995,
    "learning_rate": 0.001,
    "gamma": 0.99,
    "optimizer": "adam",
    "save_frequency": 10,
    "validation_frequency": 10,
    "validate_at_episode_1": true,
    "early_stopping_patience": 10,
    "early_stopping_metric": "total_return"
  }
}